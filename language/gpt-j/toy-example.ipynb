{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ModelComPressor(MCP) 양자화 흐름 설명\n",
    "## Quantization Scheme\n",
    "- Post-training\n",
    "- Static\n",
    "- Mixed precision\n",
    "- Linear Quantization\n",
    "\n",
    "\n",
    "    <details>\n",
    "    <summary>details</summary>\n",
    "\n",
    "    - Post-training -> No backpropagation, No weight update\n",
    "    - Static -> Not online, Observe (dynamic) range at pre-compile time, Outlier handling -> calibration -> quantization param\n",
    "    - Mixed precision -> multiple data type -> layer/operator/node-wise quantization format\n",
    "    - Linear Quantization -> Not non-linear -> scale, zero-point, round, integer\n",
    "\n",
    "    </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization Pipeline\n",
    "Load model -> calibration -> quantization\n",
    "\n",
    "실제 동작하고 accuracy 가 확인된 코드 기반으로 설명합니다. https://github.com/deeplearningfromscratch/inference/tree/mlperf-qgpt-j\n",
    "\n",
    "<img src=\"quantization flow.jpg\" width=\"1100\"/>\n",
    "\n",
    "### calibration\n",
    "\n",
    "`calibrate`\n",
    "\n",
    "https://github.com/deeplearningfromscratch/inference/blob/mlperf-qgpt-j/language/gpt-j/quantization/calibrate.py#L72\n",
    "\n",
    "- input\n",
    "    - model(`fx.GraphModule`): symbolically traced\n",
    "    - dataloader: calibration data loading, 사용자 직접 작성\n",
    "    - quantization config: calibration method, granularity, dtype, nbits, ...\n",
    "- output\n",
    "    - calibration range\n",
    "    - quantization format\n",
    "\n",
    "### quantization\n",
    "\n",
    "`quantize model`\n",
    "\n",
    "https://github.com/deeplearningfromscratch/inference/blob/mlperf-qgpt-j/language/gpt-j/quantization/__init__.py#L14\n",
    "\n",
    "- input\n",
    "    - model(`fx.GraphModule`): symbolically traced\n",
    "    - quantization config\n",
    "    - quantization parameter/calibration range\n",
    "    - quantization format\n",
    "- output\n",
    "    - quantized model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python env.\n",
    "\n",
    "```bash\n",
    "  - pip:\n",
    "      - --extra-index-url https://download.pytorch.org/whl/cu118\n",
    "      - torch==2.1.0+cu118\n",
    "      - numpy==1.26.4\n",
    "      - datasets==2.18.0\n",
    "      - nltk==3.8.1\n",
    "      - evaluate==0.4.1\n",
    "      - absl-py==2.1.0\n",
    "      - rouge_score==0.1.2\n",
    "      - git+https://github.com/furiosa-ai/model-compressor-private.git@bc5112f79809bc2c45e5a82615867f5c3b6d6265\n",
    "      - git+https://github.com/furiosa-ai/transformers-compression.git@MLPerf4.1-v1.0\n",
    "```\n",
    "\n",
    "### util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import torch\n",
    "import yaml\n",
    "from torch.fx import passes\n",
    "\n",
    "\n",
    "def load_yaml(file_path: str) -> Dict:\n",
    "    with open(file_path, \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config\n",
    "\n",
    "def vizualize_fx_graph(graph: torch.fx.GraphModule, output_path: str, name: str = \"\"):\n",
    "    g = passes.graph_drawer.FxGraphDrawer(graph, name)\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        f.write(g.get_dot_graph().create_svg())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### env. variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIG_PATH = \"toy-config.json\"\n",
    "QUANT_CONFIG_PATH = \"toy-quant_config.yaml\"\n",
    "QUANT_PARAM_PATH = \" toy-quant_param.npy\"\n",
    "QUANT_FORMAT_PATH = \"toy-quant_format.yaml\"\n",
    "\n",
    "CALIB_DATA_PATH = \"../../data/dataset/cnn-daily-mail/calibration/cnn_dailymail_calibration.json\"\n",
    "N_CALIB = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load/trace Model\n",
    "\n",
    "설명 대상 모델은 GPT-J 입니다. 분석의 편의를 위하여 GPT-J 모델에서 `n_layer` 를 28 -> 1 로 줄였습니다. 나머지 configuration 은 원본 모델과 동일합니다.\n",
    "\n",
    "text-generation 모델이 어떻게 quantize 되는지 설명합니다.\n",
    "\n",
    "```yaml\n",
    "{\n",
    "  \"_name_or_path\": \"EleutherAI/gpt-j-6b\",\n",
    "  \"activation_function\": \"gelu_new\",\n",
    "  \"architectures\": [\n",
    "    \"GPTJForCausalLM\"\n",
    "  ],\n",
    "  \"attn_pdrop\": 0.0,\n",
    "  \"bos_token_id\": 50256,\n",
    "  \"embd_pdrop\": 0.0,\n",
    "  \"eos_token_id\": 50256,\n",
    "  \"gradient_checkpointing\": false,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"layer_norm_epsilon\": 1e-05,\n",
    "  \"model_type\": \"gptj\",\n",
    "  \"n_embd\": 4096,\n",
    "  \"n_head\": 16,\n",
    "  \"n_inner\": null,\n",
    "  \"n_layer\": 1, # 28\n",
    "  \"n_positions\": 2048,\n",
    "  \"resid_pdrop\": 0.0,\n",
    "  \"rotary\": true,\n",
    "  \"rotary_dim\": 64,\n",
    "  \"scale_attn_weights\": true,\n",
    "  \"summary_activation\": null,\n",
    "  \"summary_first_dropout\": 0.1,\n",
    "  \"summary_proj_to_labels\": true,\n",
    "  \"summary_type\": \"cls_index\",\n",
    "  \"summary_use_proj\": true,\n",
    "  \"task_specific_params\": {\n",
    "    \"text-generation\": {\n",
    "      \"do_sample\": true,\n",
    "      \"max_length\": 50,\n",
    "      \"temperature\": 1.0\n",
    "    }\n",
    "  },\n",
    "  \"tie_word_embeddings\": false,\n",
    "  \"tokenizer_class\": \"GPT2Tokenizer\",\n",
    "  \"torch_dtype\": \"float32\",\n",
    "  \"transformers_version\": \"4.28.0.dev0\",\n",
    "  \"use_cache\": true,\n",
    "  \"vocab_size\": 50401\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading pre-trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "def load_pytorch_model(model_config: transformers.PretrainedConfig, use_gpu: bool=True) -> transformers.PreTrainedModel:\n",
    "    model = AutoModelForCausalLM.from_config(\n",
    "        model_config,\n",
    "        torch_dtype=torch.float32,\n",
    "    )\n",
    "\n",
    "    if use_gpu:\n",
    "        print(f\"Casting models to GPU...\")\n",
    "        assert torch.cuda.is_available(), \"torch gpu is not available, exiting...\"\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    model = model.to(memory_format=torch.channels_last)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.gptj.configuration_gptj.GPTJConfig'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTJConfig {\n",
       "  \"_name_or_path\": \"toy-config.json\",\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPTJForCausalLM\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.0,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embd_pdrop\": 0.0,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gptj\",\n",
       "  \"n_embd\": 4096,\n",
       "  \"n_head\": 4,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 1,\n",
       "  \"n_positions\": 2048,\n",
       "  \"resid_pdrop\": 0.0,\n",
       "  \"rotary\": true,\n",
       "  \"rotary_dim\": 64,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"task_specific_params\": {\n",
       "    \"text-generation\": {\n",
       "      \"do_sample\": true,\n",
       "      \"max_length\": 50,\n",
       "      \"temperature\": 1.0\n",
       "    }\n",
       "  },\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"tokenizer_class\": \"GPT2Tokenizer\",\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.31.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50401\n",
       "}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config = AutoConfig.from_pretrained(MODEL_CONFIG_PATH)\n",
    "print(type(model_config))\n",
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Casting models to GPU...\n",
      "<class 'transformers.models.gptj.modeling_gptj.GPTJForCausalLM'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTJForCausalLM(\n",
       "  (transformer): GPTJModel(\n",
       "    (wte): Embedding(50401, 4096)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=50401, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_model = load_pytorch_model(model_config)\n",
    "print(type(toy_model))\n",
    "toy_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`transformers.models.gptj.modeling_gptj.GPTJForCausalLM.forward`: https://github.com/huggingface/transformers/blob/main/src/transformers/models/gptj/modeling_gptj.py#L214\n",
    "\n",
    "<img src=\"GPT-vs-GPT-J-uai-2064x1337.png\" width=\"1100\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tracing model graph\n",
    "\n",
    "graph 구조에서 양자화를 위한 과정을 수행하기 위함\n",
    "\n",
    "`custom_symbolic_trace`: https://github.com/deeplearningfromscratch/inference/blob/mlperf-qgpt-j/language/gpt-j/quantization/custom_symbolic_trace.py#L31\n",
    "\n",
    "`HFTracer` 에 의존 https://github.com/huggingface/transformers/blob/main/src/transformers/utils/fx.py#L741"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantization.custom_symbolic_trace import custom_symbolic_trace  # isort:skip\n",
    "\n",
    "\n",
    "toy_graph, _, _ = custom_symbolic_trace(toy_model)\n",
    "vizualize_fx_graph(toy_graph, \"toy-model-f32.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<f32 graph 출력부 그래프>\n",
    "\n",
    "<img src=\"toy-model-f32.png\" width=\"1400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration\n",
    "- quantization config\n",
    "- calibration data 및 calibration data loader\n",
    "- calibration range calculation\n",
    "\n",
    "### quantization config\n",
    "acitvation 및 weight 에 대한\n",
    "- calibration method\n",
    "- quantization granularity\n",
    "- quantization data type\n",
    "- number of quantization levels(number of bits)\n",
    "\n",
    "그리고, \n",
    "\n",
    "- quantization level\n",
    "- key/value data type\n",
    "\n",
    "(SMQ 등 advanced setting 제외)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'act_calib_method': 'MINMAX_ASYM',\n",
       " 'act_dtype': 'int8',\n",
       " 'act_granularity': 'channel',\n",
       " 'act_nbits': 8,\n",
       " 'batch_size': 1,\n",
       " 'calib_batch_size': 1,\n",
       " 'model': 'gpt-j',\n",
       " 'model_overwrite': True,\n",
       " 'percentile': 99.9,\n",
       " 'target_machine': 'RGDA0',\n",
       " 'weight_calib_method': 'AMAX_SYM',\n",
       " 'weight_dtype': 'int8',\n",
       " 'weight_granularity': 'channel',\n",
       " 'weight_nbits': 8,\n",
       " 'qlevel': 4,\n",
       " 'kv_dtype': 'int8'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "qconfig = load_yaml(QUANT_CONFIG_PATH)\n",
    "qconfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calibration dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing QSL\n",
      "Encoding Samples\n",
      "Finished destroying QSL.\n",
      "<class 'torch.utils.data.dataloader.DataLoader'>\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from dataset import Dataset\n",
    "\n",
    "\n",
    "def cal_data_loader(calib_dataset_path: str, batch_size: int, n_calib: int) -> torch.utils.data.DataLoader:\n",
    "    data_object = Dataset(calib_dataset_path, batch_size)\n",
    "    data_list = []\n",
    "    for idx in range(len(data_object.source_encoded_input_ids)):\n",
    "        data_list.append(\n",
    "            {\n",
    "                \"input_ids\": data_object.source_encoded_input_ids[idx],\n",
    "                \"attention_mask\": data_object.source_encoded_attn_masks[idx],\n",
    "                \"position_ids\": torch.arange(\n",
    "                    len(data_object.source_encoded_input_ids[idx][0])\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "    return DataLoader(data_list[:n_calib], batch_size)\n",
    "\n",
    "dataloader = cal_data_loader(CALIB_DATA_PATH, batch_size=qconfig[\"calib_batch_size\"], n_calib=N_CALIB)\n",
    "print(type(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calibration range calculation(calibration)\n",
    "\n",
    "calibration 의 입력\n",
    "\n",
    "- augmented `torch.fx.GraphModule`(Qlevel2)\n",
    "- quantization config\n",
    "- calibration dataloader\n",
    "\n",
    "출력\n",
    "\n",
    "- quantization parameter\n",
    "- quantization format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantization.custom_symbolic_trace import custom_symbolic_trace  # isort:skip\n",
    "import model_compressor  # isort:skip\n",
    "\n",
    "\n",
    "def calibrate(graph: torch.fx.GraphModule, \n",
    "              qconfig: Dict, \n",
    "              calib_dataloader: torch.utils.data.DataLoader, \n",
    "              qparam_path: str, qformat_path: str) -> None:\n",
    "    # run calibration\n",
    "    model_compressor.calibrate(\n",
    "        graph,\n",
    "        model_name=qconfig[\"model\"],\n",
    "        calib_dataloader=calib_dataloader,\n",
    "        weight_calib_method=qconfig[\"weight_calib_method\"],\n",
    "        weight_granularity=qconfig[\"weight_granularity\"],\n",
    "        weight_dtype=qconfig[\"weight_dtype\"],\n",
    "        weight_nbits=qconfig[\"weight_nbits\"],\n",
    "        act_calib_method=qconfig[\"act_calib_method\"],\n",
    "        act_granularity=qconfig[\"act_granularity\"],\n",
    "        act_dtype=qconfig[\"act_dtype\"],\n",
    "        act_nbits=qconfig[\"act_nbits\"],\n",
    "        percentile=qconfig[\"percentile\"],\n",
    "        target_machine=qconfig[\"target_machine\"],\n",
    "    )\n",
    "\n",
    "    # save calibration outputs\n",
    "    model_compressor.save(\n",
    "        graph,\n",
    "        qparam_out_path=qparam_path,\n",
    "        qformat_out_path=qformat_path,\n",
    "        weight_calib_method=qconfig[\"weight_calib_method\"],\n",
    "        weight_granularity=qconfig[\"weight_granularity\"],\n",
    "        weight_dtype=qconfig[\"weight_dtype\"],\n",
    "        weight_nbits=qconfig[\"weight_nbits\"],\n",
    "        act_calib_method=qconfig[\"act_calib_method\"],\n",
    "        act_granularity=qconfig[\"act_granularity\"],\n",
    "        act_dtype=qconfig[\"act_dtype\"],\n",
    "        act_nbits=qconfig[\"act_nbits\"],\n",
    "    )\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### augmented fx graph\n",
    "\n",
    "calibration 은 일부 노드들의 weight/input 에 대하여 min/max 값을 수집하는 단계입니다.\n",
    "\n",
    "이 특정 노드에 이 역할을 진행하는 \"Calibrator\" 가 부여됩니다.\n",
    "\n",
    "기술적으로 \"dynamic & simulated quantization(fake quant)\" 그래프인 Qlevel2 그래프로 변환하는 단계입니다.\n",
    "\n",
    "calibration 단계에서는 이 일반적인 IR 을 calibration 에 사용하기 위하여 특수하게 `scale=1.0` 을 부여합니다. 즉, Identity 로 동작.\n",
    "\n",
    "이렇게 함으로써 f32 그래프에서 텐서 값들을 수집하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "not changed node: size size\n",
      "not changed node: getitem <built-in function getitem>\n",
      "not changed node: view view\n",
      "not changed node: size_1 size\n",
      "not changed node: getitem_1 <built-in function getitem>\n",
      "not changed node: view_1 view\n",
      "not changed node: long long\n",
      "not changed node: view_2 view\n",
      "not changed node: getitem_5 <built-in function getitem>\n",
      "not changed node: to to\n",
      "this module remains in legacy quant module: transformer_drop <class 'torch.nn.modules.dropout.Dropout'>\n",
      "not changed node: size_3 size\n",
      "not changed node: getitem_6 <built-in function getitem>\n",
      "not changed node: size_4 size\n",
      "not changed node: getitem_7 <built-in function getitem>\n",
      "not changed node: view_3 view\n",
      "not changed node: size_5 size\n",
      "not changed node: getitem_8 <built-in function getitem>\n",
      "not changed node: view_4 view\n",
      "not changed node: size_6 size\n",
      "not changed node: getitem_9 <built-in function getitem>\n",
      "not changed node: view_5 view\n",
      "not changed node: permute permute\n",
      "not changed node: get_embed_positions <function get_embed_positions at 0x7f144e3ec670>\n",
      "not changed node: unsqueeze unsqueeze\n",
      "not changed node: size_7 size\n",
      "not changed node: getitem_10 <built-in function getitem>\n",
      "not changed node: repeat repeat\n",
      "not changed node: gather <built-in method gather of type object at 0x7f154f710d60>\n",
      "not changed node: size_8 size\n",
      "not changed node: getitem_11 <built-in function getitem>\n",
      "not changed node: split <function split at 0x7f14e39cfe20>\n",
      "not changed node: getitem_12 <built-in function getitem>\n",
      "not changed node: getitem_13 <built-in function getitem>\n",
      "not changed node: getitem_14 <built-in function getitem>\n",
      "not changed node: getitem_15 <built-in function getitem>\n",
      "not changed node: getitem_16 <built-in function getitem>\n",
      "not changed node: getitem_17 <built-in function getitem>\n",
      "not changed node: getitem_18 <built-in function getitem>\n",
      "not changed node: repeat_interleave <built-in method repeat_interleave of type object at 0x7f154f710d60>\n",
      "not changed node: getitem_19 <built-in function getitem>\n",
      "not changed node: repeat_interleave_1 <built-in method repeat_interleave of type object at 0x7f154f710d60>\n",
      "not changed node: getitem_20 <built-in function getitem>\n",
      "not changed node: getitem_21 <built-in function getitem>\n",
      "not changed node: neg <built-in function neg>\n",
      "not changed node: stack <built-in method stack of type object at 0x7f154f710d60>\n",
      "not changed node: flatten flatten\n",
      "not changed node: getitem_24 <built-in function getitem>\n",
      "not changed node: getitem_25 <built-in function getitem>\n",
      "not changed node: neg_1 <built-in function neg>\n",
      "not changed node: stack_1 <built-in method stack of type object at 0x7f154f710d60>\n",
      "not changed node: flatten_1 flatten\n",
      "not changed node: permute_1 permute\n",
      "not changed node: permute_2 permute\n",
      "not changed node: getitem_26 <built-in function getitem>\n",
      "not changed node: getitem_27 <built-in function getitem>\n",
      "not changed node: size_9 size\n",
      "not changed node: size_10 size\n",
      "not changed node: getitem_28 <built-in function getitem>\n",
      "not changed node: transpose transpose\n",
      "not changed node: getattr_1 <built-in function getattr>\n",
      "not changed node: finfo <class 'torch.finfo'>\n",
      "not changed node: getattr_2 <built-in function getattr>\n",
      "not changed node: tensor <built-in method tensor of type object at 0x7f154f710d60>\n",
      "not changed node: getattr_4 <built-in function getattr>\n",
      "not changed node: to_1 to\n",
      "not changed node: where <built-in method where of type object at 0x7f154f710d60>\n",
      "this module remains in legacy quant module: transformer_h_0_attn_attn_dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "not changed node: permute_3 permute\n",
      "not changed node: contiguous contiguous\n",
      "not changed node: size_11 size\n",
      "not changed node: getitem_29 <built-in function getitem>\n",
      "not changed node: view_6 view\n",
      "this module remains in legacy quant module: transformer_h_0_attn_resid_dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "not changed node: pow_1 <built-in method pow of type object at 0x7f154f710d60>\n",
      "not changed node: tanh <built-in method tanh of type object at 0x7f154f710d60>\n",
      "this module remains in legacy quant module: transformer_h_0_mlp_dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "not changed node: view_7 view\n",
      "not changed node: to_2 to\n"
     ]
    }
   ],
   "source": [
    "def augment_graph(graph: torch.fx.GraphModule, \n",
    "                  calib_dataloader: torch.utils.data.DataLoader) -> torch.fx.GraphModule:\n",
    "    return model_compressor.create_quantsim_model(\n",
    "            graph,\n",
    "            weight_calib_method=qconfig[\"weight_calib_method\"],\n",
    "            weight_granularity=qconfig[\"weight_granularity\"],\n",
    "            weight_dtype=qconfig[\"weight_dtype\"],\n",
    "            weight_nbits=qconfig[\"weight_nbits\"],\n",
    "            act_calib_method=qconfig[\"act_calib_method\"],\n",
    "            act_granularity=qconfig[\"act_granularity\"],\n",
    "            act_dtype=qconfig[\"act_dtype\"],\n",
    "            act_nbits=qconfig[\"act_nbits\"],\n",
    "            qlevel=qconfig[\"qlevel\"],\n",
    "            target_machine=qconfig[\"target_machine\"],\n",
    "            dataloader=calib_dataloader,\n",
    "            disable_inout=(True, True),\n",
    "            kv_dtype=qconfig[\"kv_dtype\"],\n",
    "        )\n",
    "\n",
    "qlv2_graph = augment_graph(toy_graph, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphModule(\n",
       "  (transformer): Module(\n",
       "    (wte): ModelCompressorModuleEmbedding(\n",
       "      (org_target): Embedding(50401, 4096)\n",
       "      (_input_quantizer): TensorQuantizer(disabled)\n",
       "      (_weight_quantizer): TensorQuantizer(disabled)\n",
       "    )\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): Module(\n",
       "      (0): Module(\n",
       "        (ln_1): ModelCompressorUnaryModule(\n",
       "          (org_target): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "          (_input_quantizer): TensorQuantizer(32bit fake per-tensor amax=dynamic max=dynamic min=dynamic scale=1.0 quant)\n",
       "        )\n",
       "        (attn): Module(\n",
       "          (q_proj): ModelCompressorModuleLinear(\n",
       "            (org_target): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake axis=-1 amax=dynamic max=dynamic min=dynamic calibrator=MinMaxCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic max=dynamic min=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (k_proj): ModelCompressorModuleLinear(\n",
       "            (org_target): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake axis=-1 amax=dynamic max=dynamic min=dynamic calibrator=MinMaxCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic max=dynamic min=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (v_proj): ModelCompressorModuleLinear(\n",
       "            (org_target): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake axis=-1 amax=dynamic max=dynamic min=dynamic calibrator=MinMaxCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic max=dynamic min=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (out_proj): ModelCompressorModuleLinear(\n",
       "            (org_target): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake axis=-1 amax=dynamic max=dynamic min=dynamic calibrator=MinMaxCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic max=dynamic min=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): Module(\n",
       "          (fc_in): ModelCompressorModuleLinear(\n",
       "            (org_target): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake axis=-1 amax=dynamic max=dynamic min=dynamic calibrator=MinMaxCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic max=dynamic min=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (fc_out): ModelCompressorModuleLinear(\n",
       "            (org_target): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake axis=-1 amax=dynamic max=dynamic min=dynamic calibrator=MinMaxCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic max=dynamic min=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): ModelCompressorUnaryModule(\n",
       "      (org_target): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (_input_quantizer): TensorQuantizer(32bit fake per-tensor amax=dynamic max=dynamic min=dynamic scale=1.0 quant)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): ModelCompressorModuleLinear(\n",
       "    (org_target): Linear(in_features=4096, out_features=50401, bias=True)\n",
       "    (_input_quantizer): TensorQuantizer(disabled)\n",
       "    (_weight_quantizer): TensorQuantizer(disabled)\n",
       "  )\n",
       "  (sub): ModelCompressorBinaryModule(\n",
       "    (_input_0_quantizer): TensorQuantizer(disabled)\n",
       "    (_input_1_quantizer): TensorQuantizer(16bit fake per-tensor amax=dynamic max=dynamic min=dynamic scale=1.0 quant)\n",
       "  )\n",
       "  (mul): ModelCompressorBinaryModule(\n",
       "    (_input_0_quantizer): TensorQuantizer(16bit fake per-tensor amax=dynamic max=dynamic min=dynamic scale=1.0 quant)\n",
       "    (_input_1_quantizer): TensorQuantizer(disabled)\n",
       "  )\n",
       "  (add): ModelCompressorBinaryModule(\n",
       "    (_input_0_quantizer): TensorQuantizer(disabled)\n",
       "    (_input_1_quantizer): TensorQuantizer(disabled)\n",
       "  )\n",
       "  (add_1): ModelCompressorBinaryModule(\n",
       "    (_input_0_quantizer): TensorQuantizer(disabled)\n",
       "    (_input_1_quantizer): TensorQuantizer(disabled)\n",
       "  )\n",
       "  (add_2): ModelCompressorBinaryModule(\n",
       "    (_input_0_quantizer): TensorQuantizer(disabled)\n",
       "    (_input_1_quantizer): TensorQuantizer(disabled)\n",
       "  )\n",
       "  (add_3): ModelCompressorBinaryModule(\n",
       "    (_input_0_quantizer): TensorQuantizer(disabled)\n",
       "    (_input_1_quantizer): TensorQuantizer(disabled)\n",
       "  )\n",
       "  (floordiv): ModelCompressorBinaryModule(\n",
       "    (_input_0_quantizer): TensorQuantizer(disabled)\n",
       "    (_input_1_quantizer): TensorQuantizer(disabled)\n",
       "  )\n",
       "  (mul_1): ModelCompressorBinaryModule(\n",
       "    (_input_0_quantizer): TensorQuantizer(16bit fake per-tensor amax=dynamic max=dynamic min=dynamic scale=1.0 quant)\n",
       "    (_input_1_quantizer): TensorQuantizer(16bit fake per-tensor amax=dynamic max=dynamic min=dynamic scale=1.0 quant)\n",
       "  )\n",
       "  (mul_2): ModelCompressorBinaryModule(\n",
       "    (_input_0_quantizer): TensorQuantizer(16bit fake per-tensor amax=dynamic max=dynamic min=dynamic scale=1.0 quant)\n",
       "    (_input_1_quantizer): TensorQuantizer(16bit fake per-tensor amax=dynamic max=dynamic min=dynamic scale=1.0 quant)\n",
       "  )\n",
       "  (add_4): ModelCompressorBinaryModule(\n",
       "    (_input_0_quantizer): TensorQuantizer(16bit fake per-tensor amax=dynamic max=dynamic min=dynamic scale=1.0 quant)\n",
       "    (_input_1_quantizer): TensorQuantizer(16bit fake per-tensor amax=dynamic max=dynamic min=dynamic scale=1.0 quant)\n",
       "  )\n",
       "  (mul_3): ModelCompressorBinaryModule(\n",
       "    (_input_0_quantizer): TensorQuantizer(16bit fake per-tensor amax=dynamic max=dynamic min=dynamic scale=1.0 quant)\n",
       "    (_input_1_quantizer): TensorQuantizer(16bit fake per-tensor amax=dynamic max=dynamic min=dynamic scale=1.0 quant)\n",
       "  )\n",
       "  (mul_4): ModelCompressorBinaryModule(\n",
       "    (_input_0_quantizer): TensorQuantizer(16bit fake per-tensor amax=dynamic max=dynamic min=dynamic scale=1.0 quant)\n",
       "    (_input_1_quantizer): TensorQuantizer(16bit fake per-tensor amax=dynamic max=dynamic min=dynamic scale=1.0 quant)\n",
       "  )\n",
       "  (add_5): ModelCompressorBinaryModule(\n",
       "    (_input_0_quantizer): TensorQuantizer(16bit fake per-tensor amax=dynamic max=dynamic min=dynamic scale=1.0 quant)\n",
       "    (_input_1_quantizer): TensorQuantizer(16bit fake per-tensor amax=dynamic max=dynamic min=dynamic scale=1.0 quant)\n",
       "  )\n",
       "  (cat): ModelCompressorModuleConcat(\n",
       "    (_input_quantizer): ModuleList(\n",
       "      (0-1): 2 x TensorQuantizer(16bit fake per-tensor amax=dynamic max=dynamic min=dynamic scale=1.0 quant)\n",
       "    )\n",
       "  )\n",
       "  (cat_1): ModelCompressorModuleConcat(\n",
       "    (_input_quantizer): ModuleList(\n",
       "      (0-1): 2 x TensorQuantizer(16bit fake per-tensor amax=dynamic max=dynamic min=dynamic scale=1.0 quant)\n",
       "    )\n",
       "  )\n",
       "  (cat_2): ModelCompressorModuleConcat(\n",
       "    (_input_quantizer): ModuleList(\n",
       "      (0-1): 2 x TensorQuantizer(16bit fake per-tensor amax=dynamic max=dynamic min=dynamic scale=1.0 quant)\n",
       "    )\n",
       "  )\n",
       "  (cat_3): ModelCompressorModuleConcat(\n",
       "    (_input_quantizer): ModuleList(\n",
       "      (0-1): 2 x TensorQuantizer(16bit fake per-tensor amax=dynamic max=dynamic min=dynamic scale=1.0 quant)\n",
       "    )\n",
       "  )\n",
       "  (sub_1): ModelCompressorBinaryModule(\n",
       "    (_input_0_quantizer): TensorQuantizer(disabled)\n",
       "    (_input_1_quantizer): TensorQuantizer(disabled)\n",
       "  )\n",
       "  (matmul): ModelCompressorBinaryModule(\n",
       "    (_input_0_quantizer): TensorQuantizer(8bit fake axis=1 amax=dynamic max=dynamic min=dynamic calibrator=MinMaxCalibrator scale=1.0 quant)\n",
       "    (_input_1_quantizer): TensorQuantizer(8bit fake axis=1 amax=dynamic max=dynamic min=dynamic calibrator=MinMaxCalibrator scale=1.0 quant)\n",
       "  )\n",
       "  (truediv): ModelCompressorBinaryModule(\n",
       "    (_input_0_quantizer): TensorQuantizer(16bit fake per-tensor amax=dynamic max=dynamic min=dynamic scale=1.0 quant)\n",
       "    (_input_1_quantizer): TensorQuantizer(16bit fake per-tensor amax=dynamic max=dynamic min=dynamic scale=1.0 quant)\n",
       "  )\n",
       "  (add_6): ModelCompressorBinaryModule(\n",
       "    (_input_0_quantizer): TensorQuantizer(16bit fake per-tensor amax=dynamic max=dynamic min=dynamic scale=1.0 quant)\n",
       "    (_input_1_quantizer): TensorQuantizer(16bit fake per-tensor amax=dynamic max=dynamic min=dynamic scale=1.0 quant)\n",
       "  )\n",
       "  (softmax): ModelCompressorUnaryModule(\n",
       "    (_input_quantizer): TensorQuantizer(32bit fake per-tensor amax=dynamic max=dynamic min=dynamic scale=1.0 quant)\n",
       "  )\n",
       "  (matmul_1): ModelCompressorBinaryModule(\n",
       "    (_input_0_quantizer): TensorQuantizer(16bit fake per-tensor amax=dynamic max=dynamic min=dynamic scale=1.0 quant)\n",
       "    (_input_1_quantizer): TensorQuantizer(8bit fake axis=1 amax=dynamic max=dynamic min=dynamic calibrator=MinMaxCalibrator scale=1.0 quant)\n",
       "  )\n",
       "  (add_7): ModelCompressorBinaryModule(\n",
       "    (_input_0_quantizer): TensorQuantizer(disabled)\n",
       "    (_input_1_quantizer): TensorQuantizer(disabled)\n",
       "  )\n",
       "  (mul_5): ModelCompressorBinaryModule(\n",
       "    (_input_0_quantizer): TensorQuantizer(disabled)\n",
       "    (_input_1_quantizer): TensorQuantizer(16bit fake per-tensor amax=dynamic max=dynamic min=dynamic scale=1.0 quant)\n",
       "  )\n",
       "  (mul_6): ModelCompressorBinaryModule(\n",
       "    (_input_0_quantizer): TensorQuantizer(disabled)\n",
       "    (_input_1_quantizer): TensorQuantizer(16bit fake per-tensor amax=dynamic max=dynamic min=dynamic scale=1.0 quant)\n",
       "  )\n",
       "  (add_8): ModelCompressorBinaryModule(\n",
       "    (_input_0_quantizer): TensorQuantizer(16bit fake per-tensor amax=dynamic max=dynamic min=dynamic scale=1.0 quant)\n",
       "    (_input_1_quantizer): TensorQuantizer(16bit fake per-tensor amax=dynamic max=dynamic min=dynamic scale=1.0 quant)\n",
       "  )\n",
       "  (mul_7): ModelCompressorBinaryModule(\n",
       "    (_input_0_quantizer): TensorQuantizer(disabled)\n",
       "    (_input_1_quantizer): TensorQuantizer(16bit fake per-tensor amax=dynamic max=dynamic min=dynamic scale=1.0 quant)\n",
       "  )\n",
       "  (add_9): ModelCompressorBinaryModule(\n",
       "    (_input_0_quantizer): TensorQuantizer(disabled)\n",
       "    (_input_1_quantizer): TensorQuantizer(16bit fake per-tensor amax=dynamic max=dynamic min=dynamic scale=1.0 quant)\n",
       "  )\n",
       "  (mul_8): ModelCompressorBinaryModule(\n",
       "    (_input_0_quantizer): TensorQuantizer(16bit fake per-tensor amax=dynamic max=dynamic min=dynamic scale=1.0 quant)\n",
       "    (_input_1_quantizer): TensorQuantizer(16bit fake per-tensor amax=dynamic max=dynamic min=dynamic scale=1.0 quant)\n",
       "  )\n",
       "  (add_10): ModelCompressorBinaryModule(\n",
       "    (_input_0_quantizer): TensorQuantizer(16bit fake per-tensor amax=dynamic max=dynamic min=dynamic scale=1.0 quant)\n",
       "    (_input_1_quantizer): TensorQuantizer(16bit fake per-tensor amax=dynamic max=dynamic min=dynamic scale=1.0 quant)\n",
       "  )\n",
       "  (add_11): ModelCompressorBinaryModule(\n",
       "    (_input_0_quantizer): TensorQuantizer(16bit fake per-tensor amax=dynamic max=dynamic min=dynamic scale=1.0 quant)\n",
       "    (_input_1_quantizer): TensorQuantizer(16bit fake per-tensor amax=dynamic max=dynamic min=dynamic scale=1.0 quant)\n",
       "  )\n",
       "  (output_0_quantize_node): ModelCompressorModuleOutput(\n",
       "    (org_target): Identity()\n",
       "    (_input_quantizer): TensorQuantizer(32bit fake per-tensor amax=dynamic max=dynamic min=dynamic scale=1.0 quant)\n",
       "  )\n",
       "  (output_1_quantize_node): ModelCompressorModuleOutput(\n",
       "    (org_target): Identity()\n",
       "    (_input_quantizer): TensorQuantizer(32bit fake per-tensor amax=dynamic max=dynamic min=dynamic scale=1.0 quant)\n",
       "  )\n",
       "  (output_2_quantize_node): ModelCompressorModuleOutput(\n",
       "    (org_target): Identity()\n",
       "    (_input_quantizer): TensorQuantizer(32bit fake per-tensor amax=dynamic max=dynamic min=dynamic scale=1.0 quant)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vizualize_fx_graph(qlv2_graph, \"toy-model-qlv2.svg\")\n",
    "qlv2_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<qlv2 graph 출력부 그래프>\n",
    "\n",
    "<img src=\"toy-model-qlv2.png\" width=\"1400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그래프 특징\n",
    "\n",
    "- `ModelCompressorModuleLinear`\n",
    "- `ModelCompressorModuleConcat`\n",
    "- `ModelCompressorBinaryModule`\n",
    "- `ModelCompressorUnaryModule`\n",
    "- `ModelCompressorModuleOutput`\n",
    "- `ModelCompressorModuleEmbedding`\n",
    "\n",
    "모든 element-wise binary 연산들이 `function` 에서 `module` 로 바뀌어 있는 것을 알 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TensorQuantizer`: https://github.com/furiosa-ai/model-compressor-private/blob/main/model_compressor/quant_op/tensor_quantizer/quantizer_module.py#L37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:04<00:00, 22.69it/s]\n",
      "100%|██████████| 156/156 [00:00<00:00, 4070.80it/s]\n"
     ]
    }
   ],
   "source": [
    "calibrate(qlv2_graph, qconfig, dataloader, qparam_path=QUANT_PARAM_PATH, qformat_path=QUANT_FORMAT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration output: quant_format 과 quant_param\n",
    "- quant_format 은 각 `ModelCompressorModule(MCM)` 노드에 대한 quantization spec 을 정의합니다.\n",
    "\n",
    "    ```yaml\n",
    "    {\n",
    "        'dtype': 'int8', 'axis': 2, 'dynamic': False, \n",
    "        'etc_for_MCLab': \n",
    "            {'torch_name': 'transformer.h.0.attn.q_proj._input_quantizer', \n",
    "            'input_dtype': 'torch.float32', \n",
    "            'input_shape': [1, 1422, 4096], \n",
    "            'nbits': 8, \n",
    "            'per_ch': True, \n",
    "            'if_per_channel_scaling': False, \n",
    "            'group_size': None, \n",
    "            'unsigned': False, \n",
    "            'calibrator_type': 'minmax', \n",
    "            'calibrator_method': 'minmax', \n",
    "            'asymmetric': True, \n",
    "            'do_quant': True, \n",
    "            'do_zp_equalizing': False}\n",
    "    }\n",
    "    ```\n",
    "    \n",
    "- quant_param 은 linear quantization 을 위한 scale/zero-point 가 필요한 sub fp16 dtype 양자화 대상(예 `int8`)에 대한 (현재 맥락에서는) calibration range 가 들어 있습니다.\n",
    "\n",
    "    ```yaml\n",
    "    {\n",
    "        'amax': None,\n",
    "        'max': array([3.4669478, 3.827307 , 3.4593744, ..., 3.8100147, 3.8997357,\n",
    "            3.7047665], dtype=float32),\n",
    "        'min': array([-3.6546168, -4.5315475, -3.6790934, ..., -3.6721456, -3.8043125,\n",
    "                -3.9023168], dtype=float32),\n",
    "        'amax_outlier': None,\n",
    "        'max_outlier': None,\n",
    "        'min_outlier': None,\n",
    "        'basis': None,\n",
    "        'scale_per_channel': None,\n",
    "        'scale_per_channel_outlier': None,\n",
    "        'clipping_bound': None,\n",
    "        'outlier_cin_idx': None,\n",
    "        'ch': None\n",
    "    }\n",
    "    ```\n",
    "\n",
    "#### quant_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num: 45\n",
      "dict_keys(['transformer_wte', 'transformer_h_0_ln_1', 'transformer_h_0_attn_q_proj', 'transformer_h_0_attn_k_proj', 'transformer_h_0_attn_v_proj', 'transformer_h_0_attn_out_proj', 'transformer_h_0_mlp_fc_in', 'transformer_h_0_mlp_fc_out', 'transformer_ln_f', 'lm_head', 'sub', 'mul', 'add', 'add_1', 'add_2', 'add_3', 'floordiv', 'mul_1', 'mul_2', 'add_4', 'mul_3', 'mul_4', 'add_5', 'cat', 'cat_1', 'cat_2', 'cat_3', 'sub_1', 'matmul', 'truediv', 'add_6', 'softmax', 'matmul_1', 'add_7', 'mul_5', 'mul_6', 'add_8', 'mul_7', 'add_9', 'mul_8', 'add_10', 'add_11', 'output_0_quantize_node', 'output_1_quantize_node', 'output_2_quantize_node'])\n"
     ]
    }
   ],
   "source": [
    "qformat = load_yaml(QUANT_FORMAT_PATH)[\"quantized op list\"]\n",
    "print(f'Num: {len(qformat.keys())}')\n",
    "print(qformat.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- LayerNorm: \n",
      "{'output_shape': [1, 1422, 4096], 'quant_desc_input': {'dtype': 'fp32', 'axis': None, 'dynamic': False, 'etc_for_MCLab': {'torch_name': 'transformer.h.0.ln_1._input_quantizer', 'input_dtype': 'torch.float32', 'input_shape': [1, 1422, 4096], 'nbits': 32, 'per_ch': False, 'if_per_channel_scaling': False, 'group_size': None, 'unsigned': False, 'calibrator_type': 'None', 'calibrator_method': 'None', 'asymmetric': False, 'do_quant': True, 'do_zp_equalizing': False}}}\n",
      "\n",
      "- Add: \n",
      "{'output_shape': [0, 0, 0, 0], 'quant_desc_input_0': {'dtype': 'fp32', 'axis': None, 'dynamic': False, 'etc_for_MCLab': {'torch_name': 'add._input_0_quantizer', 'input_dtype': 'float', 'input_shape': [0], 'nbits': 32, 'per_ch': False, 'if_per_channel_scaling': False, 'group_size': None, 'unsigned': False, 'calibrator_type': 'None', 'calibrator_method': 'None', 'asymmetric': False, 'do_quant': False, 'do_zp_equalizing': False}}, 'quant_desc_input_1': {'dtype': 'fp32', 'axis': None, 'dynamic': False, 'etc_for_MCLab': {'torch_name': 'add._input_1_quantizer', 'input_dtype': 'float', 'input_shape': [0], 'nbits': 32, 'per_ch': False, 'if_per_channel_scaling': False, 'group_size': None, 'unsigned': False, 'calibrator_type': 'None', 'calibrator_method': 'None', 'asymmetric': False, 'do_quant': False, 'do_zp_equalizing': False}}}\n",
      "\n",
      "- Concat: \n",
      "{'output_shape': [1, 1422, 4, 1024], 'quant_desc_input_0': {'dtype': 'bf16', 'axis': None, 'dynamic': False, 'etc_for_MCLab': {'torch_name': 'cat._input_quantizer.0', 'input_dtype': 'torch.float32', 'input_shape': [1, 1422, 4, 64], 'nbits': 16, 'per_ch': False, 'if_per_channel_scaling': False, 'group_size': None, 'unsigned': False, 'calibrator_type': 'None', 'calibrator_method': 'None', 'asymmetric': False, 'do_quant': True, 'do_zp_equalizing': False}}, 'quant_desc_input_1': {'dtype': 'bf16', 'axis': None, 'dynamic': False, 'etc_for_MCLab': {'torch_name': 'cat._input_quantizer.1', 'input_dtype': 'torch.float32', 'input_shape': [1, 1422, 4, 960], 'nbits': 16, 'per_ch': False, 'if_per_channel_scaling': False, 'group_size': None, 'unsigned': False, 'calibrator_type': 'None', 'calibrator_method': 'None', 'asymmetric': False, 'do_quant': True, 'do_zp_equalizing': False}}}\n",
      "\n",
      "- Linear: \n",
      "{'output_shape': [1, 1422, 4096], 'quant_desc_input': {'dtype': 'int8', 'axis': 2, 'dynamic': False, 'etc_for_MCLab': {'torch_name': 'transformer.h.0.attn.q_proj._input_quantizer', 'input_dtype': 'torch.float32', 'input_shape': [1, 1422, 4096], 'nbits': 8, 'per_ch': True, 'if_per_channel_scaling': False, 'group_size': None, 'unsigned': False, 'calibrator_type': 'minmax', 'calibrator_method': 'minmax', 'asymmetric': True, 'do_quant': True, 'do_zp_equalizing': False}}, 'quant_desc_weight': {'dtype': 'int8', 'axis': 0, 'dynamic': False, 'etc_for_MCLab': {'torch_name': 'transformer.h.0.attn.q_proj._weight_quantizer', 'input_dtype': 'torch.float32', 'input_shape': [4096, 4096], 'nbits': 8, 'per_ch': True, 'if_per_channel_scaling': False, 'group_size': None, 'unsigned': False, 'calibrator_type': 'max', 'calibrator_method': 'amax', 'asymmetric': False, 'do_quant': True, 'do_zp_equalizing': False}}}\n"
     ]
    }
   ],
   "source": [
    "print(f'- LayerNorm: \\n{qformat[\"transformer_h_0_ln_1\"]}\\n')\n",
    "print(f'- Add: \\n{qformat[\"add\"]}\\n')\n",
    "print(f'- Concat: \\n{qformat[\"cat\"]}\\n')\n",
    "print(f'- Linear: \\n{qformat[\"transformer_h_0_attn_q_proj\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### quant_param\n",
    "\n",
    "양자화 대상이 되는 layer 의 양자화 통계값/매개변수\n",
    "\n",
    "- 첫번째, 마지막 layer\n",
    "- add mul 과 같은 tensor 간 element wise binary 연산\n",
    "- embedding layer\n",
    "\n",
    "에 대해서는 양자화하지 않기때문에 quant_param 이 존재하지 않습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num: 18\n",
      "dict_keys(['transformer.wte._weight_quantizer', 'transformer.h.0.attn.q_proj._input_quantizer', 'transformer.h.0.attn.q_proj._weight_quantizer', 'transformer.h.0.attn.k_proj._input_quantizer', 'transformer.h.0.attn.k_proj._weight_quantizer', 'transformer.h.0.attn.v_proj._input_quantizer', 'transformer.h.0.attn.v_proj._weight_quantizer', 'transformer.h.0.attn.out_proj._input_quantizer', 'transformer.h.0.attn.out_proj._weight_quantizer', 'transformer.h.0.mlp.fc_in._input_quantizer', 'transformer.h.0.mlp.fc_in._weight_quantizer', 'transformer.h.0.mlp.fc_out._input_quantizer', 'transformer.h.0.mlp.fc_out._weight_quantizer', 'lm_head._input_quantizer', 'lm_head._weight_quantizer', 'matmul._input_0_quantizer', 'matmul._input_1_quantizer', 'matmul_1._input_1_quantizer'])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "qparam = np.load(QUANT_PARAM_PATH, allow_pickle=True)[()]\n",
    "print(f'Num: {len(qparam.keys())}')\n",
    "print(qparam.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight ranges: \n",
      "{'amax': array([0.00256775, 0.00223099, 0.00242827, ..., 0.00225689, 0.00251386,\n",
      "       0.00288823], dtype=float32), 'max': None, 'min': None, 'amax_outlier': None, 'max_outlier': None, 'min_outlier': None, 'basis': None, 'scale_per_channel': None, 'scale_per_channel_outlier': None, 'clipping_bound': None, 'outlier_cin_idx': None, 'ch': None}\n",
      "\n",
      "Input ranges: \n",
      "{'amax': None, 'max': array([3.7416744, 3.6124158, 3.6038327, ..., 4.3889737, 3.7187653,\n",
      "       5.0028906], dtype=float32), 'min': array([-3.613177 , -3.4567056, -4.5581727, ..., -3.420187 , -3.7840812,\n",
      "       -3.5473545], dtype=float32), 'amax_outlier': None, 'max_outlier': None, 'min_outlier': None, 'basis': None, 'scale_per_channel': None, 'scale_per_channel_outlier': None, 'clipping_bound': None, 'outlier_cin_idx': None, 'ch': None}\n"
     ]
    }
   ],
   "source": [
    "print(f'Weight ranges: \\n{qparam[\"transformer.h.0.attn.q_proj._weight_quantizer\"]}\\n')\n",
    "print(f'Input ranges: \\n{qparam[\"transformer.h.0.attn.q_proj._input_quantizer\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization\n",
    "\n",
    "- `QLV4_Ops` 로 구성되어 있습니다. MCM 이 pytorch ATen 로 구성\n",
    "- 텐서들의 dtype 이 RNGD 에서 실제 구동할 dtype 으로 변환되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_model(model: torch.fx.GraphModule, \n",
    "                   qconfig: Dict, qparam_path: str, qformat_path: str) -> torch.fx.GraphModule:\n",
    "    model = model_compressor.create_quantsim_model(\n",
    "        model,\n",
    "        qformat_path=qformat_path,\n",
    "        qparam_path=qparam_path,\n",
    "        weight_calib_method=qconfig[\"weight_calib_method\"],\n",
    "        weight_granularity=qconfig[\"weight_granularity\"],\n",
    "        weight_dtype=qconfig[\"weight_dtype\"],\n",
    "        weight_nbits=qconfig[\"weight_nbits\"],\n",
    "        act_calib_method=qconfig[\"act_calib_method\"],\n",
    "        act_granularity=qconfig[\"act_granularity\"],\n",
    "        act_dtype=qconfig[\"act_dtype\"],\n",
    "        act_nbits=qconfig[\"act_nbits\"],\n",
    "        qlevel=qconfig[\"qlevel\"],\n",
    "        target_machine=qconfig[\"target_machine\"],\n",
    "        disable_inout=(True, True),\n",
    "        kv_dtype=qconfig[\"kv_dtype\"],\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "not changed node: size size\n",
      "not changed node: getitem <built-in function getitem>\n",
      "not changed node: view view\n",
      "not changed node: size_1 size\n",
      "not changed node: getitem_1 <built-in function getitem>\n",
      "not changed node: view_1 view\n",
      "not changed node: long long\n",
      "not changed node: view_2 view\n",
      "not changed node: getitem_5 <built-in function getitem>\n",
      "not changed node: to to\n",
      "this module remains in legacy quant module: transformer_drop <class 'torch.nn.modules.dropout.Dropout'>\n",
      "not changed node: size_3 size\n",
      "not changed node: getitem_6 <built-in function getitem>\n",
      "not changed node: size_4 size\n",
      "not changed node: getitem_7 <built-in function getitem>\n",
      "not changed node: view_3 view\n",
      "not changed node: size_5 size\n",
      "not changed node: getitem_8 <built-in function getitem>\n",
      "not changed node: view_4 view\n",
      "not changed node: size_6 size\n",
      "not changed node: getitem_9 <built-in function getitem>\n",
      "not changed node: view_5 view\n",
      "not changed node: permute permute\n",
      "not changed node: get_embed_positions <function get_embed_positions at 0x7f144e3ec670>\n",
      "not changed node: unsqueeze unsqueeze\n",
      "not changed node: size_7 size\n",
      "not changed node: getitem_10 <built-in function getitem>\n",
      "not changed node: repeat repeat\n",
      "not changed node: gather <built-in method gather of type object at 0x7f154f710d60>\n",
      "not changed node: size_8 size\n",
      "not changed node: getitem_11 <built-in function getitem>\n",
      "not changed node: split <function split at 0x7f14e39cfe20>\n",
      "not changed node: getitem_12 <built-in function getitem>\n",
      "not changed node: getitem_13 <built-in function getitem>\n",
      "not changed node: getitem_14 <built-in function getitem>\n",
      "not changed node: getitem_15 <built-in function getitem>\n",
      "not changed node: getitem_16 <built-in function getitem>\n",
      "not changed node: getitem_17 <built-in function getitem>\n",
      "not changed node: getitem_18 <built-in function getitem>\n",
      "not changed node: repeat_interleave <built-in method repeat_interleave of type object at 0x7f154f710d60>\n",
      "not changed node: getitem_19 <built-in function getitem>\n",
      "not changed node: repeat_interleave_1 <built-in method repeat_interleave of type object at 0x7f154f710d60>\n",
      "this module remains in legacy quant module: mul_1_quantize_node_0 <class 'model_compressor.nn.qlv4_modules.operators.qlv4_output.QLV4_Output'>\n",
      "not changed node: getitem_20 <built-in function getitem>\n",
      "not changed node: getitem_21 <built-in function getitem>\n",
      "not changed node: neg <built-in function neg>\n",
      "not changed node: stack <built-in method stack of type object at 0x7f154f710d60>\n",
      "not changed node: flatten flatten\n",
      "this module remains in legacy quant module: mul_2_quantize_node_0 <class 'model_compressor.nn.qlv4_modules.operators.qlv4_output.QLV4_Output'>\n",
      "this module remains in legacy quant module: mul_3_quantize_node_0 <class 'model_compressor.nn.qlv4_modules.operators.qlv4_output.QLV4_Output'>\n",
      "not changed node: getitem_24 <built-in function getitem>\n",
      "not changed node: getitem_25 <built-in function getitem>\n",
      "not changed node: neg_1 <built-in function neg>\n",
      "not changed node: stack_1 <built-in method stack of type object at 0x7f154f710d60>\n",
      "not changed node: flatten_1 flatten\n",
      "this module remains in legacy quant module: mul_4_quantize_node_0 <class 'model_compressor.nn.qlv4_modules.operators.qlv4_output.QLV4_Output'>\n",
      "this module remains in legacy quant module: cat_quantize_node_1 <class 'model_compressor.nn.qlv4_modules.operators.qlv4_output.QLV4_Output'>\n",
      "this module remains in legacy quant module: cat_quantize_node_0 <class 'model_compressor.nn.qlv4_modules.operators.qlv4_output.QLV4_Output'>\n",
      "this module remains in legacy quant module: cat_1_quantize_node_1 <class 'model_compressor.nn.qlv4_modules.operators.qlv4_output.QLV4_Output'>\n",
      "this module remains in legacy quant module: cat_1_quantize_node_0 <class 'model_compressor.nn.qlv4_modules.operators.qlv4_output.QLV4_Output'>\n",
      "not changed node: permute_1 permute\n",
      "not changed node: permute_2 permute\n",
      "not changed node: getitem_26 <built-in function getitem>\n",
      "not changed node: getitem_27 <built-in function getitem>\n",
      "not changed node: size_9 size\n",
      "not changed node: size_10 size\n",
      "not changed node: getitem_28 <built-in function getitem>\n",
      "not changed node: transpose transpose\n",
      "not changed node: getattr_1 <built-in function getattr>\n",
      "not changed node: finfo <class 'torch.finfo'>\n",
      "not changed node: getattr_2 <built-in function getattr>\n",
      "not changed node: tensor <built-in method tensor of type object at 0x7f154f710d60>\n",
      "not changed node: getattr_4 <built-in function getattr>\n",
      "not changed node: to_1 to\n",
      "not changed node: where <built-in method where of type object at 0x7f154f710d60>\n",
      "this module remains in legacy quant module: softmax <class 'model_compressor.nn.qlv4_modules.operators.qlv4_functional.QLV4_FSoftmax'>\n",
      "this module remains in legacy quant module: transformer_h_0_attn_attn_dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "not changed node: permute_3 permute\n",
      "not changed node: contiguous contiguous\n",
      "not changed node: size_11 size\n",
      "not changed node: getitem_29 <built-in function getitem>\n",
      "not changed node: view_6 view\n",
      "this module remains in legacy quant module: transformer_h_0_attn_out_proj_quantize_node <class 'model_compressor.nn.qlv4_modules.operators.qlv4_output.QLV4_Output'>\n",
      "this module remains in legacy quant module: transformer_h_0_attn_resid_dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "this module remains in legacy quant module: mul_5_quantize_node_1 <class 'model_compressor.nn.qlv4_modules.operators.qlv4_output.QLV4_Output'>\n",
      "not changed node: pow_1 <built-in method pow of type object at 0x7f154f710d60>\n",
      "this module remains in legacy quant module: mul_6_quantize_node_1 <class 'model_compressor.nn.qlv4_modules.operators.qlv4_output.QLV4_Output'>\n",
      "this module remains in legacy quant module: add_8_quantize_node_0 <class 'model_compressor.nn.qlv4_modules.operators.qlv4_output.QLV4_Output'>\n",
      "not changed node: tanh <built-in method tanh of type object at 0x7f154f710d60>\n",
      "this module remains in legacy quant module: transformer_h_0_mlp_dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "this module remains in legacy quant module: add_11_quantize_node_1 <class 'model_compressor.nn.qlv4_modules.operators.qlv4_output.QLV4_Output'>\n",
      "not changed node: view_7 view\n",
      "this module remains in legacy quant module: lm_head_quantize_node <class 'model_compressor.nn.qlv4_modules.operators.qlv4_output.QLV4_Output'>\n",
      "not changed node: to_2 to\n"
     ]
    }
   ],
   "source": [
    "qlv4_graph = quantize_model(toy_graph, qconfig, QUANT_PARAM_PATH, QUANT_FORMAT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphModule(\n",
       "  (transformer): Module(\n",
       "    (wte): QLV4_Embedding(\n",
       "      (QLV4_embedding): _QLV4_Embedding_MOD()\n",
       "      (_QLV4_output): QLV4_Output_MOD()\n",
       "    )\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): Module(\n",
       "      (0): Module(\n",
       "        (ln_1): QLV4_LayerNorm(\n",
       "          (_QLV4_layernorm): QLV4_LayerNorm_MOD()\n",
       "          (_QLV4_output): QLV4_Output_MOD()\n",
       "        )\n",
       "        (attn): Module(\n",
       "          (q_proj): QLV4_Linear(\n",
       "            (QLV4_linear): _QLV4_Linear_MOD()\n",
       "            (QLV4_bias): _QLV4_LINEAR_BIAS_MOD()\n",
       "            (_QLV4_output): QLV4_Output_MOD()\n",
       "          )\n",
       "          (k_proj): QLV4_Linear(\n",
       "            (QLV4_linear): _QLV4_Linear_MOD()\n",
       "            (QLV4_bias): _QLV4_LINEAR_BIAS_MOD()\n",
       "            (_QLV4_output): QLV4_Output_MOD()\n",
       "          )\n",
       "          (v_proj): QLV4_Linear(\n",
       "            (QLV4_linear): _QLV4_Linear_MOD()\n",
       "            (QLV4_bias): _QLV4_LINEAR_BIAS_MOD()\n",
       "            (_QLV4_output): QLV4_Output_MOD()\n",
       "          )\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (out_proj): QLV4_Linear(\n",
       "            (QLV4_linear): _QLV4_Linear_MOD()\n",
       "            (QLV4_bias): _QLV4_LINEAR_BIAS_MOD()\n",
       "            (_QLV4_output): QLV4_Output_MOD()\n",
       "          )\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): Module(\n",
       "          (fc_in): QLV4_Linear(\n",
       "            (QLV4_linear): _QLV4_Linear_MOD()\n",
       "            (QLV4_bias): _QLV4_LINEAR_BIAS_MOD()\n",
       "            (_QLV4_output): QLV4_Output_MOD()\n",
       "          )\n",
       "          (fc_out): QLV4_Linear(\n",
       "            (QLV4_linear): _QLV4_Linear_MOD()\n",
       "            (QLV4_bias): _QLV4_LINEAR_BIAS_MOD()\n",
       "            (_QLV4_output): QLV4_Output_MOD()\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): QLV4_LayerNorm(\n",
       "      (_QLV4_layernorm): QLV4_LayerNorm_MOD()\n",
       "      (_QLV4_output): QLV4_Output_MOD()\n",
       "    )\n",
       "  )\n",
       "  (lm_head): QLV4_Linear(\n",
       "    (QLV4_linear): _QLV4_Linear_MOD()\n",
       "    (QLV4_bias): _QLV4_LINEAR_BIAS_MOD()\n",
       "    (_QLV4_output): QLV4_Output_MOD()\n",
       "  )\n",
       "  (sub): QLV4_Sub(\n",
       "    (_QLV4_sub): QLV4_Sub_MOD()\n",
       "    (_QLV4_output): QLV4_Output_MOD()\n",
       "  )\n",
       "  (mul): QLV4_Mul(\n",
       "    (_QLV4_mul): QLV4_Mul_MOD()\n",
       "    (_QLV4_output): QLV4_Output_MOD()\n",
       "  )\n",
       "  (add): QLV4_Add(\n",
       "    (_QLV4_add): QLV4_Add_MOD()\n",
       "    (_QLV4_output): QLV4_Output_MOD()\n",
       "  )\n",
       "  (add_1): QLV4_Add(\n",
       "    (_QLV4_add): QLV4_Add_MOD()\n",
       "    (_QLV4_output): QLV4_Output_MOD()\n",
       "  )\n",
       "  (add_2): QLV4_Add(\n",
       "    (_QLV4_add): QLV4_Add_MOD()\n",
       "    (_QLV4_output): QLV4_Output_MOD()\n",
       "  )\n",
       "  (add_3): QLV4_Add(\n",
       "    (_QLV4_add): QLV4_Add_MOD()\n",
       "    (_QLV4_output): QLV4_Output_MOD()\n",
       "  )\n",
       "  (floordiv): QLV4_FloorDiv(\n",
       "    (_QLV4_floordiv): QLV4_FloorDiv_MOD()\n",
       "    (_QLV4_output): QLV4_Output_MOD()\n",
       "  )\n",
       "  (mul_1): QLV4_Mul(\n",
       "    (_QLV4_mul): QLV4_Mul_MOD()\n",
       "    (_QLV4_output): QLV4_Output_MOD()\n",
       "  )\n",
       "  (mul_2): QLV4_Mul(\n",
       "    (_QLV4_mul): QLV4_Mul_MOD()\n",
       "    (_QLV4_output): QLV4_Output_MOD()\n",
       "  )\n",
       "  (add_4): QLV4_Add(\n",
       "    (_QLV4_add): QLV4_Add_MOD()\n",
       "    (_QLV4_output): QLV4_Output_MOD()\n",
       "  )\n",
       "  (mul_3): QLV4_Mul(\n",
       "    (_QLV4_mul): QLV4_Mul_MOD()\n",
       "    (_QLV4_output): QLV4_Output_MOD()\n",
       "  )\n",
       "  (mul_4): QLV4_Mul(\n",
       "    (_QLV4_mul): QLV4_Mul_MOD()\n",
       "    (_QLV4_output): QLV4_Output_MOD()\n",
       "  )\n",
       "  (add_5): QLV4_Add(\n",
       "    (_QLV4_add): QLV4_Add_MOD()\n",
       "    (_QLV4_output): QLV4_Output_MOD()\n",
       "  )\n",
       "  (cat): QLV4_Concat(\n",
       "    (_QLV4_concat): QLV4_Concat_MOD()\n",
       "    (_QLV4_output): QLV4_Output_MOD()\n",
       "  )\n",
       "  (cat_1): QLV4_Concat(\n",
       "    (_QLV4_concat): QLV4_Concat_MOD()\n",
       "    (_QLV4_output): QLV4_Output_MOD()\n",
       "  )\n",
       "  (cat_2): QLV4_Concat(\n",
       "    (_QLV4_concat): QLV4_Concat_MOD()\n",
       "    (_QLV4_output): QLV4_Output_MOD()\n",
       "  )\n",
       "  (cat_3): QLV4_Concat(\n",
       "    (_QLV4_concat): QLV4_Concat_MOD()\n",
       "    (_QLV4_output): QLV4_Output_MOD()\n",
       "  )\n",
       "  (sub_1): QLV4_Sub(\n",
       "    (_QLV4_sub): QLV4_Sub_MOD()\n",
       "    (_QLV4_output): QLV4_Output_MOD()\n",
       "  )\n",
       "  (matmul): QLV4_MatMul(\n",
       "    (_QLV4_matmul): QLV4_MatMul_MOD()\n",
       "    (_QLV4_output): QLV4_Output_MOD()\n",
       "  )\n",
       "  (truediv): QLV4_TrueDiv(\n",
       "    (_QLV4_truediv): QLV4_TrueDiv_MOD()\n",
       "    (_QLV4_output): QLV4_Output_MOD()\n",
       "  )\n",
       "  (add_6): QLV4_Add(\n",
       "    (_QLV4_add): QLV4_Add_MOD()\n",
       "    (_QLV4_output): QLV4_Output_MOD()\n",
       "  )\n",
       "  (softmax): QLV4_FSoftmax(\n",
       "    (_QLV4_softmax): QLV4_FSoftmax_MOD()\n",
       "    (_QLV4_output): QLV4_Output_MOD()\n",
       "  )\n",
       "  (matmul_1): QLV4_MatMul(\n",
       "    (_QLV4_matmul): QLV4_MatMul_MOD()\n",
       "    (_QLV4_output): QLV4_Output_MOD()\n",
       "  )\n",
       "  (add_7): QLV4_Add(\n",
       "    (_QLV4_add): QLV4_Add_MOD()\n",
       "    (_QLV4_output): QLV4_Output_MOD()\n",
       "  )\n",
       "  (mul_5): QLV4_Mul(\n",
       "    (_QLV4_mul): QLV4_Mul_MOD()\n",
       "    (_QLV4_output): QLV4_Output_MOD()\n",
       "  )\n",
       "  (mul_6): QLV4_Mul(\n",
       "    (_QLV4_mul): QLV4_Mul_MOD()\n",
       "    (_QLV4_output): QLV4_Output_MOD()\n",
       "  )\n",
       "  (add_8): QLV4_Add(\n",
       "    (_QLV4_add): QLV4_Add_MOD()\n",
       "    (_QLV4_output): QLV4_Output_MOD()\n",
       "  )\n",
       "  (mul_7): QLV4_Mul(\n",
       "    (_QLV4_mul): QLV4_Mul_MOD()\n",
       "    (_QLV4_output): QLV4_Output_MOD()\n",
       "  )\n",
       "  (add_9): QLV4_Add(\n",
       "    (_QLV4_add): QLV4_Add_MOD()\n",
       "    (_QLV4_output): QLV4_Output_MOD()\n",
       "  )\n",
       "  (mul_8): QLV4_Mul(\n",
       "    (_QLV4_mul): QLV4_Mul_MOD()\n",
       "    (_QLV4_output): QLV4_Output_MOD()\n",
       "  )\n",
       "  (add_10): QLV4_Add(\n",
       "    (_QLV4_add): QLV4_Add_MOD()\n",
       "    (_QLV4_output): QLV4_Output_MOD()\n",
       "  )\n",
       "  (add_11): QLV4_Add(\n",
       "    (_QLV4_add): QLV4_Add_MOD()\n",
       "    (_QLV4_output): QLV4_Output_MOD()\n",
       "  )\n",
       "  (output_0_quantize_node): ModelCompressorModuleOutput(\n",
       "    (org_target): Identity()\n",
       "    (_input_quantizer): TensorQuantizer(32bit fake per-tensor amax=dynamic max=dynamic min=dynamic scale=1.0 quant)\n",
       "  )\n",
       "  (output_1_quantize_node): ModelCompressorModuleOutput(\n",
       "    (org_target): Identity()\n",
       "    (_input_quantizer): TensorQuantizer(32bit fake per-tensor amax=dynamic max=dynamic min=dynamic scale=1.0 quant)\n",
       "  )\n",
       "  (output_2_quantize_node): ModelCompressorModuleOutput(\n",
       "    (org_target): Identity()\n",
       "    (_input_quantizer): TensorQuantizer(32bit fake per-tensor amax=dynamic max=dynamic min=dynamic scale=1.0 quant)\n",
       "  )\n",
       "  (add_11_quantize_node_1): QLV4_Output(\n",
       "    (QLV4_output): QLV4_Output_MOD()\n",
       "  )\n",
       "  (mul_3_quantize_node_0): QLV4_Output(\n",
       "    (QLV4_output): QLV4_Output_MOD()\n",
       "  )\n",
       "  (mul_4_quantize_node_0): QLV4_Output(\n",
       "    (QLV4_output): QLV4_Output_MOD()\n",
       "  )\n",
       "  (cat_1_quantize_node_1): QLV4_Output(\n",
       "    (QLV4_output): QLV4_Output_MOD()\n",
       "  )\n",
       "  (mul_1_quantize_node_0): QLV4_Output(\n",
       "    (QLV4_output): QLV4_Output_MOD()\n",
       "  )\n",
       "  (mul_2_quantize_node_0): QLV4_Output(\n",
       "    (QLV4_output): QLV4_Output_MOD()\n",
       "  )\n",
       "  (cat_quantize_node_1): QLV4_Output(\n",
       "    (QLV4_output): QLV4_Output_MOD()\n",
       "  )\n",
       "  (cat_quantize_node_0): QLV4_Output(\n",
       "    (QLV4_output): QLV4_Output_MOD()\n",
       "  )\n",
       "  (cat_1_quantize_node_0): QLV4_Output(\n",
       "    (QLV4_output): QLV4_Output_MOD()\n",
       "  )\n",
       "  (transformer_h_0_attn_out_proj_quantize_node): QLV4_Output(\n",
       "    (QLV4_output): QLV4_Output_MOD()\n",
       "  )\n",
       "  (mul_5_quantize_node_1): QLV4_Output(\n",
       "    (QLV4_output): QLV4_Output_MOD()\n",
       "  )\n",
       "  (mul_6_quantize_node_1): QLV4_Output(\n",
       "    (QLV4_output): QLV4_Output_MOD()\n",
       "  )\n",
       "  (add_8_quantize_node_0): QLV4_Output(\n",
       "    (QLV4_output): QLV4_Output_MOD()\n",
       "  )\n",
       "  (lm_head_quantize_node): QLV4_Output(\n",
       "    (QLV4_output): QLV4_Output_MOD()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vizualize_fx_graph(qlv4_graph, 'toy-model-qlv4.svg')\n",
    "qlv4_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<qlv4 graph 출력부 그래프>\n",
    "\n",
    "<img src=\"toy-model-qlv4.png\" width=\"1400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "- MCM 의 양자화는 symbolically traced pytorch `torch.fx.GraphModule` 을 대상으로 합니다.\n",
    "    - 이 때, graph tracer 는 사용자가 구현해야 합니다.\n",
    "- (Qlevel2) 이 pytorch 그래프의 노드들을 `ModelCompressorModule` 로 변환하여 calibration 을 수행합니다.\n",
    "    - 이 calibration 단계에서는 각 layer 들의 dtype 등이 명시되어 있는 qformat 과 양자화를 수행하게될 layer 에 대한 qparam 을 얻을 수 있습니다.\n",
    "    - 이 과정에서 calibration 을 위한 dataloader 는 사용자가 구현해야 합니다.\n",
    "- (Qlevel4) qformat 과 qparam 을 이용하여 최종적으로 RNGD 컴파일러에서 컴파일하기 위하여 그래프를 변환 합니다.\n",
    "    - 또한 `ModelCompressorModule` 들은 모두 pytorch `ATen` ops 로 변환 됩니다.\n",
    "    - 양자화는 Qlevel2 -> Qlevel3 에서 일어납니다. 즉, 모든 layer 의 dtype 은 qformat 이 규정하는대로 dtype 을 변환하게 됩니다.\n",
    "- text-generation 모델이라고 하더라도 양자화 대상은 `model.forward` 에 포함되는 layer 들입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment\n",
    "- symbolically traced fx GraphModule 에서는 svg 형태로는 그래프 구조, data flow 를 파악하기 어려운 점이 있습니다. \n",
    "    - netron 수준의 interactive 그래프 visualizer 가 있다면 이 점을 크게 개선할 수 있을 것으로 생각합니다.\n",
    "- graph tracer, calibration dataloader 는 (아주 적은 수준이라고 하더라도) 사용자가 구현해서 사용해야할 부분입니다.\n",
    "- 양자화 관점에서는 Qlevel3 가 최종 단계이고, Qlevel4 는 이후 모델 porting(exporting) 관점에서 접근해야할 것으로 생각합니다.\n",
    "    - Qlevel3 단계에서 사용자 편의성을 위해 그래프를 serialize 해서 하나의 파일로 갖고 있을 필요가 있을 것 같습니다. \n",
    "    - (현행 양자화 모델을 실행하기 위해서는 weight, qconfig, qparam, qformat, git submodule 이 필요합니다.)\n",
    "    - 특히, text-generation 모델은 양자화 이후 pre-pill/decoder 로 나뉘어 컴파일되는만큼 양자화 도구 관점과 별도로 코드를 개발/유지보수해야할 필요성이 있습니다.\n",
    "\n",
    "끝."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.14 ('mlperf-qgpt-j')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f95bd2d7095c49e14700e94b602388f7e890ccc7bef0ce13571f75386c941c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
